{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for the TADA Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Technology Assisted Dietary Assessment (TADA) has been one of Purdue EPICS' most valuable insights for mounting nutrition intervention programs. With the growing concern about obesity, the need to accurately measure food intake has become imperative. For example, dietary assessment among adolescents is problematic as this group has irregular eating patterns and less enthusiasm for recording food intake. Preliminary studies among adolescents suggest that the innovative use of technology may improve the accuracy of dietary information from young people. Recognition of emerging advancements in technology, e.g., higher resolution pictures, improved memory capacity, faster processors, allow these devices to process information not previously possible.\n",
    "\n",
    "Our goal is to develop, implement, and evaluate a mobile device food record (mdFR) that will translate to an accurate account of daily food and nutrient intake among adolescents and adults. Our first steps include further development of our pilot mobile computing device to include digital images, a nutrient database, and image processing for identification and quantification of food consumption. Mobile computing devices provide a unique vehicle for collecting dietary information that reduces burden on record keepers. Images of food can be marked with a variety of input methods that link the item for image processing and analysis to estimate the amount of food. Images before and after foods are eaten can estimate the amount of food consumed.\n",
    "\n",
    "The Image Processing team for Fall 2017 has decided to work on three specific modules:\n",
    "1. A Convolutional Neural Network for food image recognition.\n",
    "2. A barcode scanner that provides nutritional information.\n",
    "3. Graph Based Image Segmentation for accurate food item estimation.\n",
    "\n",
    "This notebook will summarize the work done in developing the Convolutional Neural Network (CNN) for food image recognition. Primarily, the framework that we intended to use was Tensorflow. But, in order to quickly prototype our network architecture, we decided to use Keras, a Deep Learning framework that is built on top of Tensorflow and provides a high level API for users to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the necessary module imports\n",
    "\n",
    "Some of these modules may need to be installed via pip or Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "from scipy.misc import imresize, imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize class dictionaries\n",
    "\n",
    "These allow for easy mapping between a particular food type and its corresponding index and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two dictionaries to save the classes and their respective indices.\n",
    "index_to_classes = {}\n",
    "classes_to_index = {}\n",
    "\n",
    "# Adding the k-v pairs to the dicts using the custom text file.\n",
    "with open('assets/classes.txt', 'r') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "    classes_to_index = dict(zip(classes, range(len(classes))))\n",
    "    index_to_classes = dict(zip(range(len(classes)), classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Food-11 dataset.\n",
    "\n",
    "We will be using 9866 images for training, 3500 for validation, and 3000 for evaluation.\n",
    "\n",
    "NOTE: These images are loaded with the assumption that you have the dataset downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DIR = 'assets/training/'\n",
    "VALIDATION_DIR = 'assets/validation/'\n",
    "EVALUATION_DIR = 'assets/evaluation/'\n",
    "\n",
    "def load_images(root, min_side=32):\n",
    "    print('Loading ' + root[7:-1] + ' data...')\n",
    "    images = []\n",
    "    classes = []\n",
    "\n",
    "    imgs = sorted(os.listdir(root))\n",
    "\n",
    "    for img in listdir(root):\n",
    "        # print('Loading Image: ' + str(counter))\n",
    "        im = imresize(imread(root + img), (min_side, min_side))\n",
    "        arr = np.array(im)\n",
    "        images.append(arr)\n",
    "\n",
    "        if img[0:2] == '10':\n",
    "            classes.append(10)\n",
    "        else:\n",
    "            classes.append(int(img[0:1]))\n",
    "\n",
    "    return np.array(images), np.array(classes)\n",
    "\n",
    "\n",
    "# Loading the training, validation, and evaluation data.\n",
    "X_tr, Y_tr = load_images(TRAINING_DIR)\n",
    "X_val, Y_val = load_images(VALIDATION_DIR)\n",
    "X_test, Y_test = load_images(EVALUATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr = X_tr / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(np.unique(Y_tr))\n",
    "\n",
    "Y_tr = np_utils.to_categorical(Y_tr)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "\n",
    "It is a five layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the sequential model.\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the first convolution layer.\n",
    "model.add(\n",
    "    Conv2D(32,   # Number of kernels.\n",
    "           (5, 5),  # Kernel size.\n",
    "           input_shape=(32, 32, 3),\n",
    "           padding='same',\n",
    "           activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adding the first max pooling layer.\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adding the second convolutional layer.\n",
    "model.add(\n",
    "    Conv2D(64,\n",
    "           (5, 5),\n",
    "           input_shape=(16, 16, 32),\n",
    "           padding='same',\n",
    "           activation='relu')\n",
    ")\n",
    "\n",
    "# Adding the flattening layer.\n",
    "model.add(\n",
    "    Flatten()\n",
    ")\n",
    "\n",
    "# Adding the fully connected layer.\n",
    "model.add(\n",
    "    Dense(\n",
    "        n_classes,\n",
    "        activation='softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the hyperparameters and the optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 100\n",
    "l_rate = 0.001\n",
    "\n",
    "decay = l_rate / epochs\n",
    "\n",
    "sgd = SGD(\n",
    "    lr=l_rate,\n",
    "    momentum=0.9,\n",
    "    decay=decay,\n",
    "    nesterov=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add loss metrics, optimization function. Train and test the CNN.\n",
    "\n",
    "The loss function used is categorical crossentropy, and the optimization function used is Stochastic Gradient Descent.\n",
    "\n",
    "Train the model and test it after. Once the training session is complete, print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(\n",
    "    X_tr,\n",
    "    Y_tr,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "scores = model.evaluate(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\nAccuracy: %.2f%%' % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to a JSON file and the weights to a HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "\n",
    "with open('model1.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save('my_model1.h5')\n",
    "model.save_weights('model1_weights.h5')\n",
    "print('Saved model to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model1.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('model1_weights.h5')\n",
    "print('Loaded model from disk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1.0,
    "version_minor": 0.0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
